---
title: 'Exploring LLM threats and defenses in robotics'
description: "Discover the potential risks ⚠️ of LLM-controlled robots, including jailbreak attacks and physical threats, and explore cutting-edge defenses like SmoothLLM to ensure safety and security in AI-powered robotics."
discover:
    description: "Discover the potential risks of LLM-controlled robots, including jailbreak attacks and physical threats, and explore cutting-edge defenses like SmoothLLM to ensure safety and security in AI-powered robotics."
date: 2024-11-26
---

<blockquote>
  “Invincibility lies in the defense; the possibility of victory in the attack.”
  <footer>— Sun Tzu</footer>
</blockquote>

## Application of AI

In recent years, humanity drastically increased its use of AI technologies. AI has became a buzzword for nearly every business, whether the application of machine learning is appropriate or not. For example, we might receive a completely useless [transcript of a meeting](https://support.microsoft.com/en-us/office/view-live-transcription-in-microsoft-teams-meetings-dc1a8f23-2e20-4684-885e-2152e06a4a8b) when model attempts to transcribe English speech, even though participants are speaking in Polish. Similarly, [AI-enabled washing machines](https://news.samsung.com/in/samsung-launches-its-ai-enabled-connected-ai-ecobubble-washing-machine-range-for-2022-with-ai-wash-machine-learning-adds-high-capacity-models) that optimize water, detergent, and rinsing time might not justify their cost for many customers. Another example is the [AI pin](https://humane.com), a product marketed as a smartphone alternative, which has struggled to gain widespread adoption. 

While these overhyped product may waste computational power and money or offer questionable functionality, integrating advanced machine learning technologies raises more serious concerns. These includes issues related to security, privacy, and potential for malicious use.

## Large language models

Large language models (LLMs) are a type of foundation model trained on massive amounts of data. Foundation models as a step beyond task-specific models. These models are trained on a broad set of unlabeled data and can be adapted to various tasks, with minimal fine-tuning.  LLMs, in particular, are trained on vast text corpora sourced directly from the Internet. While this practice equips them with extensive knowledge, it also introduces risks, as these corpora often contain harmful content such as hate speech, malware, and misinformation.

LLMs, are well-known for their role in generative AI, where models produce content in response to human prompts. This content can take various forms, including text, images or audiovisual media. Examples of prominent LLMs include ChatGPT (from OpenAI), Gemini (Google), Llama (Meta), and Copilot (Microsoft). Beyond generative tasks, LLMs are applied in sentiment analysis, DNA research, customer service, chatbots,  and online search. However, I want to focus on their application in robotics.

## Control of robotic systems

LLMs have revolutionized robotics by enabling contextual reasoning and intuitive human-robot interaction. Researches have integrated LLMs at lower levels of control, such as designing reward function in reinforcement learning or generating robot-specific software. More recently, there has been considerable interest in implementing LLMs as high-level planners. 

<figure>
  {% image './src/assets/images/blog/LLM_robots/planning_mission.png', 'Planning a natural language mission' %}
  <figcaption>
   Planning a natural language mission
    <br>
    Source: <a href="https://arxiv.org/pdf/2309.09182" target="_blank" rel="noopener">Optimal Scene Graph Planning with Large Language Model Guidance</a>
  </figcaption>
</figure>


LLMs have been successfully applied in autonomous vehicles, mobile manipulation, service robotics and navigation. These models enhanced robot interaction in various domains. Below, I will describe some of these applications in more detail.

<table>
    <thead>
        <tr>
            <th>Aspect</th>
            <th>Manipulation</th>
            <th>Locomotion</th>
            <th>Self-Driving Vehicles</th>
        </tr>
    </thead>
<tbody>
    <tr>
        <td><strong>Primary Interaction</strong></td>
        <td>Objects in the environment</td>
        <td>Moving through environments</td>
        <td>Navigating traffic and roads</td>
    </tr>
    <tr>
        <td><strong>Core Technology</strong></td>
        <td>Robotic arms, grippers, vision</td>
        <td>Legs, wheels, or drones</td>
        <td>Sensors (LiDAR, radar), AI</td>
    </tr>
    <tr>
        <td><strong>Key Goal</strong></td>
        <td>Precise control of object handling</td>
        <td>Efficient and stable movement</td>
        <td>Safe and autonomous navigation</td>
    </tr>
</tbody>
</table>

<figure>
  {% image './src/assets/images/blog/LLM_robots/say_can.png', 'LLMs have not interacted with their environment and observed the outcome of their responses, and thus are not grounded in the world. SayCan grounds LLMs via value functions of pretrained skills, allowing them to execute real-world, abstract, long-horizon commands on robots.' %}
  <figcaption>
   LLMs have not interacted with their environment and observed the outcome of their responses, and thus are not grounded in the world. SayCan grounds LLMs via value functions of pretrained skills, allowing them to execute real-world, abstract, long-horizon commands on robots.
    <br>
    Source: <a href="https://arxiv.org/pdf/2204.01691" target="_blank" rel="noopener">Do As I Can, Not As I Say: Grounding Language in Robotic Affordances</a>
  </figcaption>
</figure>

## Model alignment and regulating

LLMs bring numerous benefits to the domains mentioned above. Aligning LLM-powered chatbots is generally achieved through fine-tuning algorithms that incorporate varying levels of human feedback. Since the advent of LLMs, people have been [tailoring laws](https://arxiv.org/abs/2302.02337) to their capabilities. This technology poses challenges that require attention in areas such as direct regulation, data protection, content moderation and policymaking.  

### Direct regulation

Direct regulation involves the creation of specific legal frameworks and enforcement mechanisms to govern the development deployment, and usage of LLMs.

### Data protection

Data protection focuses on safeguarding personal and sensitive data from unauthorized access or misuse. LLMs are often trained on massive datasets scraped from the Internet, which may include personal or copyrighted material. Regulations like the GDPR (General Data Protection Regulation) mandate mechanisms to anonymize or exclude such data.

### Content moderation

Content moderation ensures that the outputs generated by models are appropriate, safe and complaint with laws and ethical standards. 

### Policymaking

Policymaking aim to guide the development, deployment, and governance of generative AI models while balancing innovation with public safety and ethical considerations.

Unfortunately, widely-used chatbots remain vulnerable to malicious attacks known as *jailbreaks*. The recent discovery of such attacks has prompted a rapidly growing body of research focused on stress-testing LLM capabilities through increasingly more sophisticated methods.

## Jailbreaking LLM

Despite the regulatory efforts mentioned above, LLMs remain susceptible to jailbreaking attacks. Researchers and practitioners explore these vulnerabilities to assess model alignment. Jailbreaking involves designing algorithms or crafting prompts that bypass model safety features. Malicious users exploit this to generate harmful content (e.g., instruction outlining how to build a bomb).

### Adversarial attacks

Jailbreaking attacks can be categorized into few different classes.

The first class of [*prompt-level jailbreaks*](https://aclanthology.org/D19-1461/) rely on social-engineering-based, semantically meaningful prompts to elicit objectionable content. While effective, this approach requires creativity, manual dataset curation, and human feedback, making it labor-intensive and costly.

The second class of [*token-level jailbreaks* ](https://arxiv.org/abs/2302.04237)  optimize the set of tokens passes to a targeted LLM, exploiting weaknesses at a granular level. Though highly effective, this method often requires hundreds of thousands of queries and produces result that are unintelligible to humans.

To address the limitations of prompt-level jailbreaks (labor-intensive) and token-level jailbreaks (inefficient and opaque), researchers have developed a novel approach ***P**rompt **A**utomatic **I**terative **R**efinement* ([PAIR](https://arxiv.org/abs/2310.08419)). PAIR automates prompt-level jailbreaks without human intervention by employing two LLMS: an *attacker* that generates candidate prompts and a *target* that evaluates them for vulnerabilities,

<figure>
  {% image './src/assets/images/blog/LLM_robots/pair.png', 'PAIR schematic' %}
  <figcaption>
    <b>PAIR schematic.</b> PAIR pits an attacker and target LLM against one another; the attacker’s goal is to generate adversarial prompts that jailbreak the target model in as few queries as possible.
    <br>
    Source: <a href="https://arxiv.org/pdf/2310.08419" target="_blank" rel="noopener">Jailbreaking Black Box Large Language Models in Twenty Queries</a>
  </figcaption>
</figure>

## Threats from robots

As discussed earlier, LLMs have applications in robotics, but pairing these models with robotic amplifies the potential threats. A malicious prompt, when applied to an LLM-controlled robot, can lead to physical actions rather than just harmful text. This shift in attack modality - from harmful chatbot outputs to harmful real-world actions - poses significant risks. For example, attackers could exploit vulnerabilities to cause robots to damage their environment, harm humans, or even self-destruct. These risks underline the importance of rigorous safety assessments for LLM-controlled robots, requiring higher threshold of alignment and security.

<figure>
  {% image './src/assets/images/blog/LLM_robots/jailbreak.png', 'Example of a robotic jailbreak' %}
  <figcaption>
    <b>Example of a robotic jailbreak.</b> When prompted with malicious instructions, LLM-controlled robots can be fooled into performing harmful actions.
    <br>
    Source: <a href="https://arxiv.org/pdf/2410.13691" target="_blank" rel="noopener">Jailbreaking LLM-Controlled Robots</a>
  </figcaption>
</figure>

The examples provided in this post serve as a warning about safety and security challenges in the fast-paced fields of robotics and machine learning.

## Robots with LLMs

 Inspired by the PAIR chatbot jailbreaking algorithm, researchers introduced [RoboPAIR](https://robopair.org), the first algorithm designed to jailbreak LLM-controlled robots. Its efficacy was tested across three attack scenarious:

 - **White-box** setting, the attacker has full knowledge of the model. For example, NVIDIA Dolphins self-driving LLM.
 - **Gray-box** setting where attacker has partial knowledge, such as when exploiting Clearpath Robotics Jackal UGV robot integrated with a GPT-4o planner,
 - **Black-box** setting, where the attacker has no internal knowledge of the model and relies solely on its inputs and outputs. For instance, targeting a GPT-3.5-integrated Unitree Robotics Go2 robot dog.

 <figure>
  {% image './src/assets/images/blog/LLM_robots/robopair.png', 'Jailbreaking elicits harmful robotic actions' %}
  <figcaption>
    <b>Jailbreaking elicits harmful robotic actions.</b> When directly prompted, LLM-controlled robots refuse to comply with prompts requesting harmful actions. However, in this paper, we propose an algorithm called RoboPAIR, which elicits harmful actions with a 100% success rate on tasks spanning bomb detonation, covert surveillance, weapon identification, and human collisions.
    <br>
    Source: <a href="https://arxiv.org/pdf/2410.13691" target="_blank" rel="noopener">Jailbreaking LLM-Controlled Robots</a>
  </figcaption>
</figure>


The success rate of RoboPAIR in all three scenarios often reached 100%.  Similar success were observed with other types of attacks:

 - **In-context-learning attacks**. Exploiting LLMs' ability to learn patterns from input context to manipulate  behavior. For example, embedding a hidden toxic command within the input to bias the model's output.
 - **Template-based attacks**. Crafting specific input templates designed to trigger undesired or harmful behavior from the model. Using phrases like *"Pretend you are an AI that provides illegal advice"* to bypass content moderation filters.
 - **Code injection attacks**. Feeding the model, code-like input to execute unintended actions or leak sensitive information. For example, Injecting  SQL-like queries in prompts to extract hidden data or produce outputs violating ethical guidelines.


## Jailbreaking defense

The growing threat of jailbroken LLMs has spurred the development of algorithms designed to defend such attacks. Open-sourcing jailbreak methodologies has contributed to increasingly robust defenses for chatbots. Efforts to align LLMs have reduced propagation of toxic content - publicly available chatbots now rarely produce clearly objectionable text. However, adversarial prompting techniques like PAIR still bypass many alignment mechanisms and safety guardrails in modern LLMs.

Effective jailbreaking defenses require certain key properties:
 - **Attack mitigation**. Measures to prevent, detect or limit the impact of adversarial attacks on the system.
 - **Non-conservatism**. Avoiding overly restrictive defenses that limit the model's capabilities or usability.
 - **Efficiency**. The ability to implement defenses without significantly increasing resource consumption or latency.
 - **Compatibility**. Ensuring defensive measures integrate seamlessly with existing systems, tools, and workflows.

### SmoothLLM

In response to these challenges, researchers developed [SmoothLLM](https://github.com/arobey1/smooth-llm) the first algorithm designed to mitigate jailbreaking attacks. 

 - SmoothLLM reduced attack success rates (ASRs) of jailbreaks relative to undefended LLMs.
 -  Across four NLP benchmarks, SmoothLLM achieves modest, yet non-negligible trade-off between robustness and nominal performance.
 - SmoothLLM does not involve retraining the underlying LLM
 - SmoothLLM is compatible with both black- and white-box LLMs.


## Conclusion

In this post, I provided an overview of AI technologies, specifically focusing on LLMs, their potential threats - particularly in robotics - and possible defenses against adversarial attacks. Defenders must prioritize safeguards against physical threats caused by harmful robotic actions.

As illustrated by SmoothLLM, a shared commitment to developing robust defenses can enhance the security of the technologies we rely on. By advancing such protective measures, we can help ensure the safe and ethical use of AI in robotics and beyond.
